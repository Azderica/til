---
sidebar_position: 9
---

# 9. 웹 크롤러 설계

- 크롤러는 다양하게 이용됩니다.
  - 검색 엔전 인덱싱(search engine indexing): 크롤러의 가장 보편적인 용례
  - 웹 아카이빙(web archiving): 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
  - 웹 마이닝(web mining): 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출할 수 있습니다.
  - 웹 모니터링(web monitoring): 크롤러를 통해 저작권이나 상표권이 침해되는 사례를 모니터링

## 1단계 문제 이해 및 설계 범위 확정

- 웹 크롤러의 기본 알고리즘은 다음과 같습니다.
  - (1) URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드 합니다.
  - (2) 다운받은 웹 페이지에 URL들을 추출합니다.
  - (3) 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복합니다.
- 다음의 요구사항이 있습니다.
  - 규모 확장성: 웹은 거대합니다.
  - 안정성(robustness): 비정상적인 입력이나 환경에 잘 대응할 수 있어야 합니다.
  - 예절(politeness): 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안됩니다.
  - 확장성(extensibility): 새로운 형태의 콘텐츠를 지원하기가 쉬워야 합니다.

### 개략적 규모 추정

- 매달 10억 개의 웹 페이지를 다운로드 합니다.
- QPS = 10억/30일/24시간/3600초 = 대략 400페이지/초
- 최대(Peak) QPS = 2 * QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 * 500k = 500TB/월
- 1개월치 데이터를 보관하는데 500TB, 5년간 보관한다고 가정시 500TB * 12개월 * 5sus = 30PB의 저장용량이 필요합니다.

<br/>

## 2단계 개략적 설계안 제시 및 동의 구하기

<br/>

## 3단계 상세 설계

<br/>

## 4단계 마무리