---
sidebar_position: 9
---

# 9. 웹 크롤러 설계

- 크롤러는 다양하게 이용됩니다.
  - 검색 엔전 인덱싱(search engine indexing): 크롤러의 가장 보편적인 용례
  - 웹 아카이빙(web archiving): 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차
  - 웹 마이닝(web mining): 웹 마이닝을 통해 인터넷에서 유용한 지식을 도출할 수 있습니다.
  - 웹 모니터링(web monitoring): 크롤러를 통해 저작권이나 상표권이 침해되는 사례를 모니터링

## 1단계 문제 이해 및 설계 범위 확정

- 웹 크롤러의 기본 알고리즘은 다음과 같습니다.
  - (1) URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드 합니다.
  - (2) 다운받은 웹 페이지에 URL들을 추출합니다.
  - (3) 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복합니다.
- 다음의 요구사항이 있습니다.
  - 규모 확장성: 웹은 거대합니다.
  - 안정성(robustness): 비정상적인 입력이나 환경에 잘 대응할 수 있어야 합니다.
  - 예절(politeness): 크롤러는 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내면 안됩니다.
  - 확장성(extensibility): 새로운 형태의 콘텐츠를 지원하기가 쉬워야 합니다.

### 개략적 규모 추정

- 매달 10억 개의 웹 페이지를 다운로드 합니다.
- QPS = 10억/30일/24시간/3600초 = 대략 400페이지/초
- 최대(Peak) QPS = 2 * QPS = 800
- 웹 페이지의 크기 평균은 500k라고 가정
- 10억 페이지 * 500k = 500TB/월
- 1개월치 데이터를 보관하는데 500TB, 5년간 보관한다고 가정시 500TB * 12개월 * 5sus = 30PB의 저장용량이 필요합니다.

<br/>

## 2단계 개략적 설계안 제시 및 동의 구하기

![개략적 설계](https://user-images.githubusercontent.com/42582516/186928449-612b2dfe-dc8b-4fbe-9dda-11d94d38509b.png)

- 시작 URL 집합
  - 시작 URL 집합은 웹 크롤러가 크롤링을 시작하는 출발점입니다.
- 미수집 URL 저장소
  - FIFO 큐 라고 생각하면 됩니다.
- HTML 다운로더
  - HTML 다운로더는 인터넷에서 웹 페이지를 다운로드하는 컴포넌트입니다.
- 도메인 이름 변환기
  - 웹 페이지를 다운받으려면 URL을 IP 주소로 변환하는 절차가 필요합니다.
- 콘텐츠 파서
  - 웹 페이지를 다운로드하면 파싱(parsing)과 검증(validation) 절차를 거쳐야 합니다.
- 중복 콘텐츠인가?
  - 웹 페이지 해시 값을 비교하여 중복 컨텐츠를 줄일 수 있습니다.
- 콘텐츠 저장소
  - 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장합니다.
  - 인기 있는 콘텐츠는 메모리에 두어 접근 지연시간을 줄일 수 있습니다.
- URL 추출기
  - URL 추출기는 HTML 페이지를 파싱하여 링크들을 골라내는 역할을 합니다.
- URL 필터
  - URL 필터는 특정한 콘텐츠 타입이나 파일 확장자를 갖는 URL, 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL 등을 크롤링 대상에서 배제하는 역할을 합니다.
- 이미 방문한 URL?
  - 이미 방문한 URL이나 미수집 URL 저장소에 보관된 URL을 추적할 수 있는 자료 구조를 사용합니다.
  - 블룸 필터(bloom filter)나 해시 테이블이 널리 쓰입니다.
- URL 저장소
  - 이미 방문한 URL을 보관하는 저장소입니다.
- 웹 크롤러 작업 흐름
  - 위의 화살표 순으로 진행합니다.

<br/>

## 3단계 상세 설계

<br/>

## 4단계 마무리