---
sidebar_position: 9
---

# 9. 웹 로봇

자동으로 웹 사이트들을 탐색하며 그 방식에 따라 '크롤러', '스파이더', '웜', 봇' 등의 다양한 이름으로 불립니다.

## 9.1 크롤러와 크롤링

- 웹 크롤러는 웹 페이지 하나를 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오는 일 등을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용합니다.

### 어디에서 시작하는가: '루트 집합'

크롤러를 풀기 위해서는 출발지점을 주어야하며, 크롤러가 방문을 시작하는 URL들의 초기 집합을 **루트 집합(root set)** 이라고 부릅니다.

![image](https://user-images.githubusercontent.com/42582516/132419472-c98add6d-0dde-4b69-a91a-b1fc3e9fdfec.png)

일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려져있지 않은 페이지들의 목록으로 구성됩니다.

### 링크 추출과 상대 링크 정상화

크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색하며, 각 페이지안의 URL 링크를 파싱하여 크롤링 페이지들의 목록에 추가합니다.

### 순환 피하기

로봇들은 순환을 피하기 위해서 반드시 그들이 어디를 방문했는지에 대해 알아야합니다. 순환은 로봇을 함정에 빠트려 멈추게나 혹은 진행을 느리게 합니다.

### 루프와 중복

순환은 크롤러에게 다음의 단점을 제공합니다.

- 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 합니다.
- 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 됩니다.
- 비록 루프 자체가 문제가 되지 않아도, 많은 수의 중복된 페이지(dups)들을 가져오게 됩니다.

### 빵 부스러기의 흔적

전세계의 웹 URL을 방문했는지 추적하려면 복잡한 자료구조를 사용할 필요가 있으며, 이 자료 구조는 속도와 메모리 사용 면에서 효과적이어야합니다.

- 트리와 해시 테이블 : 복잡한 로봇의 경우, 검색 트리나 해시 테이블의 사용합니다.
- 느슨한 존재 비트맵 : 공간 사용을 최소화하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용합니다.
- 체크 포인트 : 로봇의 갑작스러운 중단에 대비해 방문한 URL 목록이 디스크에 저장되었는지 확인합니다.
- 파티셔닝 : 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링이 불가능하므로, 몇몇 대규모 웹 로봇은 농장(farm)을 이용하며 각 로봇은 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 집니다.

### 별칭(alias)과 로봇 순화

올바른 자료 구조가 있더라도 URL이 별칭을 가질 수 있는 이상, 이를 방문했는지에 대해 쉽지 않을 때가 있습니다.

### URL 정규화

대부분의 웹 로봇은 URL들이 표준 형식으로 `정규화`함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려고 시도합니다.

1. 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가합니다.
2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환합니다.
3. `#태그`들을 제거합니다.

### 파일 시스템 링크 순환

- 파일 시스템의 심벌릭 링크는 의미도 없고, 깊어지는 디렉터리 계층이 생길 수 있습니다, (교묘한 종류의 순환)
- 이러한 루프를 발견하지 못하면 URL의 길이가 로봇이나 서버의 한계를 넘을 때까지 이 순환이 계속됩니다.

### 동적 가상 웹 공간

- 다만 위의 페이지 순환을 감지하기는 매우 어렵습니다.
- 대표적으로 로봇은 달력의 다음달을 계속 누를수도 있습니다. (인간과 다르므로)

### 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법이 없으며, 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로합니다.

다음의 기법을 주로 사용합니다.

- URL 정규화 : URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복된 URL을 회피합니다.
- 너비 우선 크롤링 : URL들의 큰집합에서 너비 우선으로 스케줄링하면 순환의 영향이 적어집니다.
- 스로틀링 : 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한합니다.
- URL 크기 제한 : 로봇은 일정 길이를 넘는 URL의 크롤링을 거부할 수 있습니다. (보통 1KB) 추가적으로 에러로그를 통해 어느 사이트에서 문제가 발생하는지 확인할 수 있습니다.
- URL/사이트 블랙리스트 : 함정 사이트에 대한 URL 목록을 만듭니다.
- 패턴 발견 : 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르므로 이러한 반복 구성요소를 크롤링하는 것을 거절합니다.
- 콘텐츠 지문(fingerprint) : 콘텐츠에서 몇 바이트를 얻어내 체크섬(checksum)을 계산하고 이를 통해서 중복을 방지합니다. 중복이 적은 체크섬을 사용하며 보통 MD5와 같은 메시지 요약 함수가 인기 있습니다.
- 사람의 모니터링 : 위의 기법을 적용해도 해결이 안되는 부분은 사람의 모니터링을 통해 해결합니다.

<br/>

## 9.2 로봇의 HTTP

<br/>

## 9.3 부적절하게 동작하는 로봇들

<br/>

## 9.4 로봇 차단하기

<br/>

## 9.5 로봇 에티켓

<br/>

## 9.6 로봇 엔진
