---
sidebar_position: 9
---

# 9. 웹 로봇

자동으로 웹 사이트들을 탐색하며 그 방식에 따라 '크롤러', '스파이더', '웜', 봇' 등의 다양한 이름으로 불립니다.

## 9.1 크롤러와 크롤링

- 웹 크롤러는 웹 페이지 하나를 가져오고, 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오는 일 등을 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용합니다.

### 어디에서 시작하는가: '루트 집합'

크롤러를 풀기 위해서는 출발지점을 주어야하며, 크롤러가 방문을 시작하는 URL들의 초기 집합을 **루트 집합(root set)** 이라고 부릅니다.

![image](https://user-images.githubusercontent.com/42582516/132419472-c98add6d-0dde-4b69-a91a-b1fc3e9fdfec.png)

일반적으로 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려져있지 않은 페이지들의 목록으로 구성됩니다.

### 링크 추출과 상대 링크 정상화

크롤러는 웹을 돌아다니면서 꾸준히 HTML 문서를 검색하며, 각 페이지안의 URL 링크를 파싱하여 크롤링 페이지들의 목록에 추가합니다.

### 순환 피하기

로봇들은 순환을 피하기 위해서 반드시 그들이 어디를 방문했는지에 대해 알아야합니다. 순환은 로봇을 함정에 빠트려 멈추게나 혹은 진행을 느리게 합니다.

### 루프와 중복

순환은 크롤러에게 다음의 단점을 제공합니다.

- 순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 합니다.
- 크롤러가 같은 페이지를 반복해서 가져오면 웹 서버의 부담이 됩니다.
- 비록 루프 자체가 문제가 되지 않아도, 많은 수의 중복된 페이지(dups)들을 가져오게 됩니다.

### 빵 부스러기의 흔적

전세계의 웹 URL을 방문했는지 추적하려면 복잡한 자료구조를 사용할 필요가 있으며, 이 자료 구조는 속도와 메모리 사용 면에서 효과적이어야합니다.

- 트리와 해시 테이블 : 복잡한 로봇의 경우, 검색 트리나 해시 테이블의 사용합니다.
- 느슨한 존재 비트맵 : 공간 사용을 최소화하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열과 같은 느슨한 자료 구조를 사용합니다.
- 체크 포인트 : 로봇의 갑작스러운 중단에 대비해 방문한 URL 목록이 디스크에 저장되었는지 확인합니다.
- 파티셔닝 : 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링이 불가능하므로, 몇몇 대규모 웹 로봇은 농장(farm)을 이용하며 각 로봇은 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 집니다.

### 별칭(alias)과 로봇 순화

올바른 자료 구조가 있더라도 URL이 별칭을 가질 수 있는 이상, 이를 방문했는지에 대해 쉽지 않을 때가 있습니다.

### URL 정규화

대부분의 웹 로봇은 URL들이 표준 형식으로 `정규화`함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려고 시도합니다.

1. 포트 번호가 명시되지 않았다면, 호스트 명에 ':80'을 추가합니다.
2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환합니다.
3. `#태그`들을 제거합니다.

### 파일 시스템 링크 순환

- 파일 시스템의 심벌릭 링크는 의미도 없고, 깊어지는 디렉터리 계층이 생길 수 있습니다, (교묘한 종류의 순환)
- 이러한 루프를 발견하지 못하면 URL의 길이가 로봇이나 서버의 한계를 넘을 때까지 이 순환이 계속됩니다.

### 동적 가상 웹 공간

- 다만 위의 페이지 순환을 감지하기는 매우 어렵습니다.
- 대표적으로 로봇은 달력의 다음달을 계속 누를수도 있습니다. (인간과 다르므로)

### 루프와 중복 피하기

모든 순환을 피하는 완벽한 방법이 없으며, 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로합니다.

다음의 기법을 주로 사용합니다.

- URL 정규화 : URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복된 URL을 회피합니다.
- 너비 우선 크롤링 : URL들의 큰집합에서 너비 우선으로 스케줄링하면 순환의 영향이 적어집니다.
- 스로틀링 : 로봇이 웹 사이트에서 일정 시간 동안 가져올 수 있는 페이지의 숫자를 제한합니다.
- URL 크기 제한 : 로봇은 일정 길이를 넘는 URL의 크롤링을 거부할 수 있습니다. (보통 1KB) 추가적으로 에러로그를 통해 어느 사이트에서 문제가 발생하는지 확인할 수 있습니다.
- URL/사이트 블랙리스트 : 함정 사이트에 대한 URL 목록을 만듭니다.
- 패턴 발견 : 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르므로 이러한 반복 구성요소를 크롤링하는 것을 거절합니다.
- 콘텐츠 지문(fingerprint) : 콘텐츠에서 몇 바이트를 얻어내 체크섬(checksum)을 계산하고 이를 통해서 중복을 방지합니다. 중복이 적은 체크섬을 사용하며 보통 MD5와 같은 메시지 요약 함수가 인기 있습니다.
- 사람의 모니터링 : 위의 기법을 적용해도 해결이 안되는 부분은 사람의 모니터링을 통해 해결합니다.

<br/>

## 9.2 로봇의 HTTP

- 로봇들도 HTTP 명세의 규칙을 지켜야합니다.
- 많은 로봇이 그들이 찾는 콘텐츠를 요청하기 위해 필요한 HTTP을 최소한으로만 구현하려고 합니다.

### 요청 헤더 식별하기

- 로봇들도 대부분은 약간의 신원 식별 헤더를 구현하고 전송합니다.
- 로봇을 나타내는 정보를 기본적인 헤더를 사이트에게 보내주는 것이 좋습니다.
  - `User-Agent` : 서버에게 요청을 만든 로봇의 이름을 말해줍니다.
  - `From` : 로봇의 사용자/관리자의 이메일 주소를 제공합니다.
  - `Accept` : 서버에게 어떤 미디어 타입을 보내도 되는지 말해줍니다.
  - `Referer` : 현재 요청 URL을 포함한 문서의 URL을 제공합니다.

### 가상 호스팅

- 로봇 구현자들은 Host 헤더를 지원할 필요가 있습니다.
- 대부분의 서버들은 기본적으로 특정 사이트 하나를 운영하도록 설정되어 있습니다.

### 조건부 요청

- 로봇이 검색하는 콘텐츠의 양을 최소화하는 것은 의미있습니다.
- 로봇의 몇몇은 시간이나 엔티티 태그를 비교함으로써 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현합니다.

### 응답 다루기

대다수 로봇은 주 관심사가 단순히 GET 메서드 콘텐츠를 요청해서 가져오므로 응답은 거의 받지 않습니다. 그렇나 일부 로봇은 HTTP 응답을 다룰 필요가 있습니다.

#### 상태 코드

- 로봇들은 최소한 일반적인 상태코드나 예상할 수 있는 상태 코드를 다룰 수 있어야합니다. (200 OK, 404 Not Found)
- 모든 서버가 언제나 항상 적절한 에러 코드를 반환하지 않기때문에 조심해야합니다.

#### 엔티티

- HTTP 헤더에 임베딩된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있습니다.
- 몇몇 서버는 HTML 페이지를 보내기 전에 그 내용을 파싱하여 http-equiv 태그를 헤더로 포함시킵니다.

### `User-Agent` 타기팅

- 많은 웹 사이트들은 그들의 여러 기능을 지원할 수 있도록 브라우저의 종류를 감지하여 그에 맞게 콘텐츠를 최적화합니다.
- 사이트 관리자들은 로봇이 사이트를 방문했을때 콘텐츠를 제공 못하는 경우를 방지해야합니다.

<br/>

## 9.3 부적절하게 동작하는 로봇들

로봇이 통제를 읽으면 다음의 문제를 만들 수 있습니다.

### 폭주하는 로봇

- 로봇이 논리적인 에러나 순환에 빠졌다면 웹 서버에 극심한 부하를 줄 수 있습니다.
- 반드시 로봇을 설계할 때는 폭주 방지를 설계해야합니다.

### 오래된 URL

- 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있고 이에 따른 에러 로그가 많이 발생할 수 있습니다.

### 길고 잘못된 URL

- 크고 의미 없는 URL을 요청할 수 있습니다.
- URL이 긴 경우, 웹 서버의 처리 능력에 영향을 주고 웹 서버 로그를 어지럽게 합니다.

### 호기심이 지나친 로봇

- 사적인 데이터를 쉽게 접근하도록 하는 것은 사생활 침해로 볼 수 있습니다.
- 웹에서 사이트 구현자들이 사이트 구현자들이 원하지 않는 데이터를 접근하지 않도록 하는 것이 좋습니다.

### 동적 게이트웨어 접근

- 로봇이 게이트웨어 애플리케이션의 콘텐츠에 대한 URL로 요청하는 것은 좋지 않은 상황입니다.

<br/>

## 9.4 로봇 차단하기

<br/>

## 9.5 로봇 에티켓

<br/>

## 9.6 로봇 엔진
